# -*- coding: utf-8 -*-
"""생체인증보안-hw1-김윤서.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OCCNNLMS17hbHzK0gjz7yJLGha7Lnelt

#  Increase the performance of Accuracy
### -tensorflow-

## 성능 향상을 위해 변경한 변수들
1. 노드 개수 (num_units): 2,4,6, ...
2. 경사하강법의 학습율(GradientDescentOptimizer): 0.001, 0.0005, 0.0001, 0.00001...
3. 반복 횟수: 1000, 4000, 10000...
4. 은닉계층 활성화 함수: tanh, relu, ...

### 1. 필요한 모듈 불러오기
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from numpy.random import multivariate_normal, permutation
import pandas as pd
from pandas import DataFrame, Series

"""### 2. 텐서플로우 버전 낮추기"""

import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

"""### 3. 난수의 씨드를 설정"""

np.random.seed(1971063)
tf.set_random_seed(1971063)

"""### 4. 학습 데이터 생성하기"""

def generate_datablock(n, mu, var, t):
  data= multivariate_normal(mu, np.eye(2)*var, n)
  df= DataFrame(data, columns=['x1','x2'])
  df['t'] = t
  return df

        ### 기존 데이터 ###
#df0 = generate_datablock(15, [7,7], 22, 0)
#df1 = generate_datablock(15, [22,7], 22, 0)
#df2 = generate_datablock(10, [7,22], 22, 0)
#df3 = generate_datablock(25, [20,20], 22, 1)

        ### 새로운 데이터 ###
df0 = generate_datablock(1500, [7,7], 22, 1)
df1 = generate_datablock(1500, [22,7], 22, 0)
df2 = generate_datablock(1500, [7,22], 22, 1)
df3 = generate_datablock(1500, [22,22], 22, 0)

df = pd.concat([df0, df1, df2, df3], ignore_index=True)
train_set = df.reindex(permutation(df.index)).reset_index(drop=True)

"""### 5. 학습 데이터 확인하기"""

train_set

"""#### 6. (x1, x2)와 t를 각각 모은 것을 넘파이의 array 오브젝트로 추출해둔다."""

train_x = train_set[['x1','x2']].to_numpy()
train_t = train_set['t'].to_numpy().reshape([len(train_set), 1])

"""### 7. 단층 신경망을 이용한 이항 분류기 모델을 정의한다.
: 가설함수로 단층 신경망 기반 이항 분류기 모델을 정의함.   
#### <변수 설정>
- **layer** = 4
- **활성화 함수**: relu 사용
"""

num_units = 4  # 노드 개수: 4
mult = train_x.flatten().mean() # x1,x2의 평균값= mult

x = tf.placeholder(tf.float32, [None, 2])

w1 = tf.Variable(tf.truncated_normal([2, num_units])) # 난수 발생
b1 = tf.Variable(tf.zeros([num_units]))

hidden1 = tf.nn.relu(tf.matmul(x, w1)+ b1*mult)  #relu로도 해보고
#hidden1 = tf.nn.tanh(tf.matmul(x, w1) + b1*mult) # tanh로도 해보기--> 성능 뭐가 더?

w0 = tf.Variable(tf.zeros([num_units, 1]))
b0 = tf.Variable(tf.zeros([1]))
p = tf.nn.sigmoid(tf.matmul(hidden1, w0) + b0*mult) #여기도 mult 곱해주는것...

""" *mult 에 대하여--   
 bias는 결국 데이터의 평균값(mult)을 따라가므로, *mult 해준다.
#### --> 파라미터 최적화를 고속화 하기 위한 처리임!
"""

p.shape

"""### 8. Loss 함수(비용함수,오차함수), 학습 알고리즘(단계), 정답률 식 정의
* 오차함수: loss
* 트레이닝 알고리즘: train_step
* 정답률 accuracy
#### <변수 설정>
##### 경사하강법의 학습율(GradientDescentOptimizer)= **0.000008**
"""

t = tf.placeholder(tf.float32, [None, 1])
loss = -tf.reduce_sum(t*tf.log(p) + (1-t)*tf.log(1-p))
train_step = tf.train.GradientDescentOptimizer(0.00001).minimize(loss) # 이 부분
correct_prediction = tf.equal(tf.sign(p-0.5), tf.sign(t-0.5))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

"""### 9. 세션을 준비하고 Variable을 초기화"""

sess = tf.InteractiveSession()
sess.run(tf.global_variables_initializer())

"""### 10. 파라미터 최적화를 **5000**회 반복한다."""

# Commented out IPython magic to ensure Python compatibility.
i = 0
for _ in range(5000): #5000번 반복
  i+=1
  sess.run(train_step, feed_dict={x:train_x, t:train_t}) #feed_dict로 파라미터 넘겨준다!!
  if i % 100 == 0:  ##100번 단위로 출력해준다
    loss_val, acc_val = sess.run(
        [loss, accuracy], feed_dict={x:train_x, t:train_t})
    print('Step: %d, Loss: %f, Accuracy: %f'
#     %(i,loss_val, acc_val))  ## 반복횟수,오차,정확도 출력

"""### 11. 얻어진 확률을 색의 농담으로 그림에 표시한다."""

train_set1 = train_set[train_set['t']==1]
train_set2 = train_set[train_set['t']==0]

fig = plt.figure(figsize=(6,6))
subplot = fig.add_subplot(1,1,1)
subplot.set_ylim([0,30])
subplot.set_xlim([0,30])
subplot.scatter(train_set1.x1, train_set1.x2, marker='x')
subplot.scatter(train_set2.x1, train_set2.x2, marker='o')

locations = []
for x2 in np.linspace(0,30,100):
  for x1 in np.linspace(0,30,100):
    locations.append((x1, x2))
p_vals = sess.run(p, feed_dict={x:locations})
p_vals = p_vals.reshape((100,100))
subplot.imshow(p_vals, origin='lower', extent=(0,30,0,30),
               cmap=plt.cm.gray_r, alpha=0.5)

"""# 결과
**정확도 0.946667, 오차 849.5** 정도로 도출
####      
### <변수 설정>
1. 노드 개수 = **4**
2. GradientDescentOptimizer= **0.000008**
3. 반복 횟수= **5,000**
4. **relu** 함수 사용
"""

